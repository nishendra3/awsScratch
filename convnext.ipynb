{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27547dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_file(source_directory, destination_directory, filename):\n",
    "    \"\"\"\n",
    "    Utility function used to copy a file from a source_directory to a destination_directory\n",
    "    \"\"\"\n",
    "    destination_directory.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy(source_directory/filename, destination_directory/filename)\n",
    "    \n",
    "def organize_train_valid_dataset(root, dest, labels, valid_probability=0.1):\n",
    "    \"\"\"\n",
    "    Creates the train, train_valid and valid folders respecting PyTorch's ImageDataset structure, performing\n",
    "    train/validation split based on the given percentage\n",
    "    \"\"\"\n",
    "    source_directory = root/'images'\n",
    "    \n",
    "    for index, row in labels.iterrows():\n",
    "        img_index = row['filename'].split('.')[0]  # The filename is the name of the image except the extension\n",
    "        img_class = row['quantity']\n",
    "\n",
    "        # Randomly assign the image to the valid dataset with probability 'valid_probability'\n",
    "        channel = Path('train') if random()>valid_probability else Path('valid')\n",
    "        destination_directory = dest/channel/str(img_class)\n",
    "\n",
    "        # Copy the image to either the train or valid folder, and also to the train_valid folder\n",
    "        copy_file(source_directory, destination_directory, row['filename'])\n",
    "        copy_file(source_directory, dest/'train_valid'/str(img_class), row['filename'])\n",
    "\n",
    "def organize_test_dataset(root, dest, labels):\n",
    "    \"\"\"\n",
    "    Creates the test folder respecting PyTorch's ImageDataset structure, using a dummy 'undefined' label\n",
    "    \"\"\"\n",
    "    source_directory = root/'images'\n",
    "        \n",
    "    for index, row in labels.iterrows():\n",
    "        img_index = row['filename'].split('.')[0]  # The index is the name of the image except the extension\n",
    "\n",
    "        channel = Path('test')\n",
    "        destination_directory = dest/channel/'undefined'\n",
    "        \n",
    "        try:\n",
    "            copy_file(source_directory, destination_directory, row['filename'])\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edc17cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path('./SageMaker')\n",
    "dest = Path('./SageMaker')\n",
    "dest.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Read in the labels DataFrame with a label for each image\n",
    "train = pd.read_csv(f'{root}/train.csv')\n",
    "test = pd.read_csv(f'{root}/test.csv')\n",
    "\n",
    "# Create the train/train_valid/valid folder structure\n",
    "valid_probability = 0.1\n",
    "organize_train_valid_dataset(root, dest, train, valid_probability)\n",
    "\n",
    "# Create the test folder structure\n",
    "organize_test_dataset(root, dest, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "359025c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "\n",
    "train_transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224,224)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "train_dataset, train_valid_dataset = [torchvision.datasets.ImageFolder(folder, transform=train_transforms) for folder in [root/'train', root/'train_valid']]\n",
    "\n",
    "\n",
    "valid_transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224,224)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "valid_dataset, test_dataset = [torchvision.datasets.ImageFolder(folder, transform=valid_transforms) for folder in [root/'valid', root/'test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e43ab70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "print(num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3d9ae037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2*num_gpus, pin_memory=True)\n",
    "train_valid_dataloader = torch.utils.data.DataLoader(train_valid_dataset, batch_size=128, shuffle=True, num_workers=2*num_gpus, pin_memory=True)\n",
    "\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=256, shuffle=False, num_workers=2*num_gpus, pin_memory=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=2*num_gpus, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e6fa8459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "def get_net():\n",
    "    convnext_tiny = timm.create_model('convnext_tiny', pretrained=True)\n",
    "    num_ftrs = convnext_tiny.head.fc.in_features\n",
    "    convnext_tiny.head.fc = torch.nn.Linear(in_features=num_ftrs, out_features=6, bias=True)\n",
    "    \n",
    "    #resnext = torchvision.models.vit_b_16(pretrained=True)\n",
    "    \n",
    "    #resnext.fc = torch.nn.Linear(num_ftrs, 6)\n",
    "    #resnext.classifier[6] = torch.nn.Linear(resnext.classifier[6].in_features, 6)\n",
    "    torch.nn.init.xavier_uniform_(convnext_tiny.head.fc.weight) # initialize the weights of the new layer\n",
    "    return convnext_tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f766b7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(net, train_dataloader, valid_dataloader, criterion, optimizer, scheduler=None, epochs=10, device='cpu', checkpoint_epochs=10):\n",
    "    start = time.time()\n",
    "    print(f'Training for {epochs} epochs on {device}')\n",
    "    \n",
    "    for epoch in range(1,epochs+1):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        \n",
    "        net.train()  # put network in train mode for Dropout and Batch Normalization\n",
    "        train_loss = torch.tensor(0., device=device)  # loss and accuracy tensors are on the GPU to avoid data transfers\n",
    "        train_accuracy = torch.tensor(0., device=device)\n",
    "        for X, y in train_dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            preds = net(X)\n",
    "            loss = criterion(preds, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                train_loss += loss * train_dataloader.batch_size\n",
    "                train_accuracy += (torch.argmax(preds, dim=1) == y).sum()\n",
    "        \n",
    "        if valid_dataloader is not None:\n",
    "            net.eval()  # put network in train mode for Dropout and Batch Normalization\n",
    "            valid_loss = torch.tensor(0., device=device)\n",
    "            valid_accuracy = torch.tensor(0., device=device)\n",
    "            with torch.no_grad():\n",
    "                for X, y in valid_dataloader:\n",
    "                    X = X.to(device)\n",
    "                    y = y.to(device)\n",
    "                    preds = net(X)\n",
    "                    loss = criterion(preds, y)\n",
    "\n",
    "                    valid_loss += loss * valid_dataloader.batch_size\n",
    "                    valid_accuracy += (torch.argmax(preds, dim=1) == y).sum()\n",
    "        \n",
    "        if scheduler is not None: \n",
    "            scheduler.step()\n",
    "            \n",
    "        print(f'Training loss: {train_loss/len(train_dataloader.dataset):.2f}')\n",
    "        print(f'Training accuracy: {100*train_accuracy/len(train_dataloader.dataset):.2f}')\n",
    "        \n",
    "        if valid_dataloader is not None:\n",
    "            print(f'Valid loss: {valid_loss/len(valid_dataloader.dataset):.2f}')\n",
    "            print(f'Valid accuracy: {100*valid_accuracy/len(valid_dataloader.dataset):.2f}')\n",
    "        \n",
    "        if epoch%checkpoint_epochs==0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': net.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }, './checkpoint.pth.tar')\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f'Total training time: {end-start:.1f} seconds')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7f5d832c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10 epochs on cuda\n",
      "Epoch 1/10\n",
      "Training loss: 1.34\n",
      "Training accuracy: 37.80\n",
      "Valid loss: 1.22\n",
      "Valid accuracy: 43.58\n",
      "\n",
      "Epoch 2/10\n",
      "Training loss: 1.12\n",
      "Training accuracy: 49.90\n",
      "Valid loss: 1.14\n",
      "Valid accuracy: 48.76\n",
      "\n",
      "Epoch 3/10\n",
      "Training loss: 0.94\n",
      "Training accuracy: 59.01\n",
      "Valid loss: 1.22\n",
      "Valid accuracy: 46.06\n",
      "\n",
      "Epoch 4/10\n",
      "Training loss: 0.67\n",
      "Training accuracy: 73.45\n",
      "Valid loss: 1.32\n",
      "Valid accuracy: 47.20\n",
      "\n",
      "Epoch 5/10\n",
      "Training loss: 0.31\n",
      "Training accuracy: 89.85\n",
      "Valid loss: 1.74\n",
      "Valid accuracy: 46.73\n",
      "\n",
      "Epoch 6/10\n",
      "Training loss: 0.10\n",
      "Training accuracy: 97.73\n",
      "Valid loss: 2.28\n",
      "Valid accuracy: 47.20\n",
      "\n",
      "Epoch 7/10\n",
      "Training loss: 0.04\n",
      "Training accuracy: 99.22\n",
      "Valid loss: 2.49\n",
      "Valid accuracy: 46.75\n",
      "\n",
      "Epoch 8/10\n",
      "Training loss: 0.03\n",
      "Training accuracy: 99.44\n",
      "Valid loss: 2.60\n",
      "Valid accuracy: 45.94\n",
      "\n",
      "Epoch 9/10\n",
      "Training loss: 0.03\n",
      "Training accuracy: 99.27\n",
      "Valid loss: 2.75\n",
      "Valid accuracy: 45.56\n",
      "\n",
      "Epoch 10/10\n",
      "Training loss: 0.04\n",
      "Training accuracy: 99.05\n",
      "Valid loss: 2.87\n",
      "Valid accuracy: 46.73\n",
      "\n",
      "Total training time: 4520.0 seconds\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lr, epochs = 1e-4, 10\n",
    "\n",
    "# get network and move it to GPU\n",
    "net = get_net().to(device)\n",
    "\n",
    "# standard CrossEntropy Loss for multi-class classification problems\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# define the optimizer for the network parameters\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "# start training process\n",
    "net = train(net, train_dataloader, valid_dataloader, criterion, optimizer, None, epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f241f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor to accumulate all predictions in\n",
    "all_outputs = torch.tensor([], device=device)\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for X, _ in test_dataloader:\n",
    "        X = X.to(device)\n",
    "        pred = net(X).argmax(dim=1).type(torch.float32) # keep the class with highest probability\n",
    "        all_outputs = torch.cat((all_outputs, pred), 0) # concatenate predictions to the list of all predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1e7c328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the submission dataframe\n",
    "test = pd.read_csv('./SageMaker/solution.csv')\n",
    "test['quantity'] = all_outputs.type(torch.int32).cpu().numpy() # move tensor to CPU and convert to numpy arrays\n",
    "\n",
    "# write the csv file\n",
    "test[['index', 'quantity']].to_csv('./SageMaker/pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693fa0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
